{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole-v0 \n",
    "# Shahid Gulzar Padder\n",
    "# XPXSKK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.999 )the system  has converged  after  690  episodes.\n",
      "0.8 )the system  has converged  after  371  episodes.\n",
      "0.7 )the system  has converged  after  134  episodes.\n",
      "0.6 )the system  has converged  after  720  episodes.\n",
      "0.5 )the system  has converged  after  277  episodes.\n",
      "1\n",
      "0.999 )the system  has converged  after  508  episodes.\n",
      "0.8 )the system  has converged  after  417  episodes.\n",
      "0.7 )the system  has converged  after  962  episodes.\n",
      "0.6 )the system  has converged  after  591  episodes.\n",
      "0.5 )the system  has converged  after  693  episodes.\n",
      "2\n",
      "0.999 )the system  has converged  after  282  episodes.\n",
      "0.8 )the system  has converged  after  479  episodes.\n",
      "0.7 )the system  has converged  after  872  episodes.\n",
      "0.6 )the system  has converged  after  675  episodes.\n",
      "0.5 )the system  has converged  after  854  episodes.\n",
      "3\n",
      "0.999 )the system  has converged  after  270  episodes.\n",
      "0.8 )the system  has converged  after  525  episodes.\n",
      "0.7 )the system  has converged  after  628  episodes.\n",
      "0.6 )the system  has converged  after  932  episodes.\n",
      "0.5 )the system  has converged  after  1394  episodes.\n",
      "4\n",
      "0.999 )the system  has converged  after  743  episodes.\n",
      "0.8 )the system  has converged  after  473  episodes.\n",
      "0.7 )the system  has converged  after  442  episodes.\n",
      "0.6 )the system  has converged  after  3913  episodes.\n",
      "0.5 )the system  has converged  after  692  episodes.\n",
      "5\n",
      "0.999 )the system  has converged  after  633  episodes.\n",
      "0.8 )the system  has converged  after  391  episodes.\n",
      "0.7 )the system  has converged  after  2089  episodes.\n",
      "0.6 )the system  has converged  after  1009  episodes.\n",
      "0.5 )the system  has converged  after  324  episodes.\n",
      "6\n",
      "0.999 )the system  has converged  after  544  episodes.\n",
      "0.8 )the system  has converged  after  443  episodes.\n",
      "0.7 )the system  has converged  after  876  episodes.\n",
      "0.6 )the system  has converged  after  601  episodes.\n",
      "0.5 )the system  has converged  after  365  episodes.\n",
      "7\n",
      "0.999 )the system  has converged  after  616  episodes.\n",
      "0.8 )the system  has converged  after  528  episodes.\n",
      "0.7 )the system  has converged  after  893  episodes.\n",
      "0.6 )the system  has converged  after  481  episodes.\n",
      "0.5 )the system  has converged  after  466  episodes.\n",
      "8\n",
      "0.999 )the system  has converged  after  242  episodes.\n",
      "0.8 )the system  has converged  after  312  episodes.\n",
      "0.7 )the system  has converged  after  1059  episodes.\n",
      "0.6 )the system  has converged  after  1148  episodes.\n",
      "0.5 )the system  has converged  after  880  episodes.\n",
      "9\n",
      "0.999 )the system  has converged  after  391  episodes.\n",
      "0.8 )the system  has converged  after  279  episodes.\n",
      "0.7 )the system  has converged  after  1165  episodes.\n",
      "0.6 )the system  has converged  after  1148  episodes.\n",
      "0.5 )the system  has converged  after  508  episodes.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Q-learning algorithm implementation.\"\"\"\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import gym\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_eps_greedy_policy(Q, eps, n):\n",
    "    \"\"\"Define a policy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : array_like (n_states x n_actions)\n",
    "        Action-values.\n",
    "    eps : float\n",
    "        Eps-greedy factor.\n",
    "    n : integer\n",
    "        Number of actions.\n",
    "    Returns\n",
    "    -------\n",
    "    policy : function\n",
    "        Function that returns actions given an input state (the present state).\n",
    "    \"\"\"\n",
    "    def policy(state):\n",
    "        \"\"\"Define a set of actions.\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : tuple\n",
    "            Present state on which depends our next action.\n",
    "        Retuns\n",
    "        ------\n",
    "        A : array\n",
    "            Probabilities for actions in the set of possible actions to be taken.\n",
    "        \"\"\"\n",
    "        A = np.ones(n, dtype=float) * eps/n\n",
    "        best = np.argmax(Q[state])\n",
    "        A[best] += 1 - eps\n",
    "        return A\n",
    "    return policy\n",
    "\n",
    "def q_learning_control(env, max_num_episodes, discount=1.0, eps=0.99, alpha=0.05):\n",
    "    \"\"\"Q-learning control.\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : OpenAI gym environment\n",
    "        Environment which will be used in the simulation (CartPole-v0).\n",
    "    max_num_episodes : int\n",
    "        Max number of episodes to converge.\n",
    "    discount : float\n",
    "        Discount factor.\n",
    "    eps : float\n",
    "        Exploration rate (since we are using an epsilon-greedy policy).\n",
    "    eps_decay : float\n",
    "        Exploration rate decay over episodes.\n",
    "    eps_min : float\n",
    "        Min exploration rate reachable.\n",
    "    Returns\n",
    "    -------\n",
    "    converged : bool\n",
    "        True if the algorithm converged, False otherwise.\n",
    "    num_episodes : int\n",
    "        Number of episodes to converge.\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    possible_actions = np.arange(env.action_space.n)\n",
    "    converged = False\n",
    "    returns = []\n",
    "\n",
    "    for num_episodes in range(max_num_episodes):\n",
    "        totalreward = 0     # total reward in this episodes (+1 each step)\n",
    "        state = build_state(env.reset())\n",
    "        policy = make_eps_greedy_policy(Q, eps, env.action_space.n)\n",
    "\n",
    "        for t in itertools.count():\n",
    "            probs = policy(state)\n",
    "            action = rnd.choice(possible_actions, p=probs)\n",
    "\n",
    "            # take the current action and observe the reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = build_state(next_state)\n",
    "\n",
    "            # if the cartpole system fell down during this episode\n",
    "            if (done):\n",
    "                # high penalization helps the convergence\n",
    "                Q[state][action] += -200\n",
    "                totalreward += reward\n",
    "                returns.append(totalreward)\n",
    "                break\n",
    "\n",
    "            # update Q-values using Q-learning update rule\n",
    "            max_action = np.argmax(Q[next_state])\n",
    "            Q[state][action] = Q[state][action] + alpha*(reward + discount*Q[next_state][max_action] - Q[state][action])\n",
    "\n",
    "            # append this step's reward\n",
    "            totalreward += reward\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # we are not decaying in the sensibility analysis\n",
    "        #if i%100 == 0:\n",
    "        #    eps *= eps_decay\n",
    "        #    if (eps < eps_min):\n",
    "        #        eps = eps_min\n",
    "\n",
    "        # winning condition: last 100 episodes have a mean of WINNING_MEAN total reward\n",
    "        mean = np.mean(returns[-100:])\n",
    "        if mean >= WINNING_MEAN:\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "    return converged, num_episodes\n",
    "\n",
    "def build_state(state):\n",
    "    \"\"\"Discretize the state returned by the environment.\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : tuple\n",
    "        State returned by OpenAI gym environment.\n",
    "    Returns\n",
    "    -------\n",
    "    _ : tuple\n",
    "        The correspondent discrete state.\n",
    "    \"\"\"\n",
    "    return (np.digitize([state[0]], cart_position_bins)[0],\n",
    "            np.digitize([state[1]], cart_velocity_bins)[0],\n",
    "            np.digitize([state[2]], pole_angle_bins)[0],\n",
    "            np.digitize([state[3]], angle_rate_bins)[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v0')\n",
    "    max_num_episodes = 6000\n",
    "    discount = 0.999\n",
    "    eps = 0.001\n",
    "    #eps_decay = 0.0\n",
    "    #eps_min = 0.01\n",
    "    alpha = 0.2\n",
    "\n",
    "    NUM_ITERATIONS = 10\n",
    "    WINNING_MEAN = 170\n",
    "    EPS_ANALYSIS = 0\n",
    "    ALPHA_ANALYSIS = 1\n",
    "    DISCOUNT_ANALYSIS = 2\n",
    "    mode = DISCOUNT_ANALYSIS\n",
    "\n",
    "    # number of discrete states\n",
    "    n_bins = 8\n",
    "    n_bins_angle = 10\n",
    "\n",
    "    # discrete states for each variable\n",
    "    cart_position_bins = pandas.cut([-2.4, 2.4], bins=n_bins, retbins=True)[1][1:-1]\n",
    "    cart_velocity_bins = pandas.cut([-1, 1], bins=n_bins, retbins=True)[1][1:-1]\n",
    "    pole_angle_bins = pandas.cut([-2, 2], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "    angle_rate_bins = pandas.cut([-3.5, 3.5], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "\n",
    "    array_eps_to_conv = []\n",
    "    if (mode == EPS_ANALYSIS):\n",
    "        eval_array = [0.001,0.003,0.006,0.01,0.013]\n",
    "    elif (mode == ALPHA_ANALYSIS):\n",
    "        eval_array = [0.1,0.2,0.3,0.5,0.7]\n",
    "    else:\n",
    "        eval_array = [0.999,0.8,0.7,0.6,0.5]\n",
    "\n",
    "    for t in range(NUM_ITERATIONS):\n",
    "        array_temp = []\n",
    "        print(t)\n",
    "\n",
    "        for hyperparameter in eval_array:\n",
    "            if (mode == EPS_ANALYSIS):\n",
    "                has_converged, episodes_to_converge = q_learning_control(env, max_num_episodes, discount, hyperparameter, alpha)\n",
    "            elif (mode == ALPHA_ANALYSIS):\n",
    "                has_converged, episodes_to_converge = q_learning_control(env, max_num_episodes, discount, eps, hyperparameter)\n",
    "            else:\n",
    "                has_converged, episodes_to_converge = q_learning_control(env, max_num_episodes, hyperparameter, eps, alpha)\n",
    "\n",
    "            array_temp.append(episodes_to_converge)\n",
    "            print(hyperparameter, \")the system \", \"has converged\" if (has_converged) else \"hasn't converged\", \" after \", episodes_to_converge, \" episodes.\")\n",
    "\n",
    "        array_eps_to_conv.append(array_temp)\n",
    "\n",
    "    array_eps_to_conv = np.array(array_eps_to_conv)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    if (mode == EPS_ANALYSIS):\n",
    "        title = r'$\\gamma = {:.3f}; \\alpha = {:.3f}$'.format(discount, alpha)\n",
    "        label = r'$\\epsilon$'\n",
    "        name = 'eps_sensibility.png'\n",
    "        ax.set_ylim((0,2000))\n",
    "    elif (mode == ALPHA_ANALYSIS):\n",
    "        title = r'$\\gamma = {:.3f}; \\epsilon = {:.3f}$'.format(discount, eps)\n",
    "        label = r'$\\alpha$'\n",
    "        name = 'lr_sensibility.png'\n",
    "        ax.set_ylim((0,1000))\n",
    "    else:\n",
    "        title = r'$\\alpha = {:.3f}; \\epsilon = {:.3f}$'.format(alpha, eps)\n",
    "        label = r'$1-\\gamma$'\n",
    "        eval_array = [np.around(1-disc, decimals=4) for disc in eval_array]\n",
    "        name = 'discount_sensibility.png'\n",
    "        ax.set_ylim((0,1500))\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('episodes to converge')\n",
    "    ax.set_xlabel(label)\n",
    "    ax.boxplot(array_eps_to_conv, labels=eval_array)\n",
    "    plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.001 )the system  has converged  after  504  episodes.\n",
      "0.003 )the system  has converged  after  813  episodes.\n",
      "0.006 )the system  has converged  after  875  episodes.\n",
      "0.01 )the system  has converged  after  454  episodes.\n",
      "0.013 )the system  has converged  after  1239  episodes.\n",
      "1\n",
      "0.001 )the system  has converged  after  379  episodes.\n",
      "0.003 )the system  has converged  after  742  episodes.\n",
      "0.006 )the system  has converged  after  527  episodes.\n",
      "0.01 )the system  has converged  after  922  episodes.\n",
      "0.013 )the system  has converged  after  451  episodes.\n",
      "2\n",
      "0.001 )the system  has converged  after  744  episodes.\n",
      "0.003 )the system  has converged  after  416  episodes.\n",
      "0.006 )the system  has converged  after  700  episodes.\n",
      "0.01 )the system  has converged  after  1400  episodes.\n",
      "0.013 )the system  has converged  after  300  episodes.\n",
      "3\n",
      "0.001 )the system  has converged  after  699  episodes.\n",
      "0.003 )the system  has converged  after  574  episodes.\n",
      "0.006 )the system  has converged  after  1034  episodes.\n",
      "0.01 )the system  has converged  after  1101  episodes.\n",
      "0.013 )the system  has converged  after  673  episodes.\n",
      "4\n",
      "0.001 )the system  has converged  after  763  episodes.\n",
      "0.003 )the system  has converged  after  472  episodes.\n",
      "0.006 )the system  has converged  after  1050  episodes.\n",
      "0.01 )the system  has converged  after  798  episodes.\n",
      "0.013 )the system  has converged  after  2009  episodes.\n",
      "5\n",
      "0.001 )the system  has converged  after  790  episodes.\n",
      "0.003 )the system  has converged  after  1317  episodes.\n",
      "0.006 )the system  has converged  after  751  episodes.\n",
      "0.01 )the system  has converged  after  975  episodes.\n",
      "0.013 )the system  has converged  after  1601  episodes.\n",
      "6\n",
      "0.001 )the system  has converged  after  800  episodes.\n",
      "0.003 )the system  has converged  after  766  episodes.\n",
      "0.006 )the system  has converged  after  338  episodes.\n",
      "0.01 )the system  has converged  after  1047  episodes.\n",
      "0.013 )the system  has converged  after  738  episodes.\n",
      "7\n",
      "0.001 )the system  has converged  after  360  episodes.\n",
      "0.003 )the system  has converged  after  764  episodes.\n",
      "0.006 )the system  has converged  after  831  episodes.\n",
      "0.01 )the system  has converged  after  293  episodes.\n",
      "0.013 )the system  has converged  after  722  episodes.\n",
      "8\n",
      "0.001 )the system  has converged  after  782  episodes.\n",
      "0.003 )the system  has converged  after  407  episodes.\n",
      "0.006 )the system  has converged  after  745  episodes.\n",
      "0.01 )the system  has converged  after  585  episodes.\n",
      "0.013 )the system  has converged  after  705  episodes.\n",
      "9\n",
      "0.001 )the system  has converged  after  254  episodes.\n",
      "0.003 )the system  has converged  after  446  episodes.\n",
      "0.006 )the system  has converged  after  1299  episodes.\n",
      "0.01 )the system  has converged  after  737  episodes.\n",
      "0.013 )the system  has converged  after  651  episodes.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Q-learning algorithm implementation.\"\"\"\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import gym\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_eps_greedy_policy(Q, eps, n):\n",
    "    \"\"\"Define a policy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : array_like (n_states x n_actions)\n",
    "        Action-values.\n",
    "    eps : float\n",
    "        Eps-greedy factor.\n",
    "    n : integer\n",
    "        Number of actions.\n",
    "    Returns\n",
    "    -------\n",
    "    policy : function\n",
    "        Function that returns actions given an input state (the present state).\n",
    "    \"\"\"\n",
    "    def policy(state):\n",
    "        \"\"\"Define a set of actions.\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : tuple\n",
    "            Present state on which depends our next action.\n",
    "        Retuns\n",
    "        ------\n",
    "        A : array\n",
    "            Probabilities for actions in the set of possible actions to be taken.\n",
    "        \"\"\"\n",
    "        A = np.ones(n, dtype=float) * eps/n\n",
    "        best = np.argmax(Q[state])\n",
    "        A[best] += 1 - eps\n",
    "        return A\n",
    "    return policy\n",
    "\n",
    "def q_learning_control(env, max_num_episodes, discount=1.0, eps=0.99, alpha=0.05):\n",
    "    \"\"\"Q-learning control.\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : OpenAI gym environment\n",
    "        Environment which will be used in the simulation (CartPole-v0).\n",
    "    max_num_episodes : int\n",
    "        Max number of episodes to converge.\n",
    "    discount : float\n",
    "        Discount factor.\n",
    "    eps : float\n",
    "        Exploration rate (since we are using an epsilon-greedy policy).\n",
    "    eps_decay : float\n",
    "        Exploration rate decay over episodes.\n",
    "    eps_min : float\n",
    "        Min exploration rate reachable.\n",
    "    Returns\n",
    "    -------\n",
    "    converged : bool\n",
    "        True if the algorithm converged, False otherwise.\n",
    "    num_episodes : int\n",
    "        Number of episodes to converge.\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    possible_actions = np.arange(env.action_space.n)\n",
    "    converged = False\n",
    "    returns = []\n",
    "\n",
    "    for num_episodes in range(max_num_episodes):\n",
    "        totalreward = 0     # total reward in this episodes (+1 each step)\n",
    "        state = build_state(env.reset())\n",
    "        policy = make_eps_greedy_policy(Q, eps, env.action_space.n)\n",
    "\n",
    "        for t in itertools.count():\n",
    "            probs = policy(state)\n",
    "            action = rnd.choice(possible_actions, p=probs)\n",
    "\n",
    "            # take the current action and observe the reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = build_state(next_state)\n",
    "\n",
    "            # if the cartpole system fell down during this episode\n",
    "            if (done):\n",
    "                # high penalization helps the convergence\n",
    "                Q[state][action] += -200\n",
    "                totalreward += reward\n",
    "                returns.append(totalreward)\n",
    "                break\n",
    "\n",
    "            # update Q-values using Q-learning update rule\n",
    "            max_action = np.argmax(Q[next_state])\n",
    "            Q[state][action] = Q[state][action] + alpha*(reward + discount*Q[next_state][max_action] - Q[state][action])\n",
    "\n",
    "            # append this step's reward\n",
    "            totalreward += reward\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # we are not decaying in the sensibility analysis\n",
    "        #if i%100 == 0:\n",
    "        #    eps *= eps_decay\n",
    "        #    if (eps < eps_min):\n",
    "        #        eps = eps_min\n",
    "\n",
    "        # winning condition: last 100 episodes have a mean of WINNING_MEAN total reward\n",
    "        mean = np.mean(returns[-100:])\n",
    "        if mean >= WINNING_MEAN:\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "    return converged, num_episodes\n",
    "\n",
    "def build_state(state):\n",
    "    \"\"\"Discretize the state returned by the environment.\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : tuple\n",
    "        State returned by OpenAI gym environment.\n",
    "    Returns\n",
    "    -------\n",
    "    _ : tuple\n",
    "        The correspondent discrete state.\n",
    "    \"\"\"\n",
    "    return (np.digitize([state[0]], cart_position_bins)[0],\n",
    "            np.digitize([state[1]], cart_velocity_bins)[0],\n",
    "            np.digitize([state[2]], pole_angle_bins)[0],\n",
    "            np.digitize([state[3]], angle_rate_bins)[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v0')\n",
    "    max_num_episodes = 6000\n",
    "    discount = 0.999\n",
    "    eps = 0.001\n",
    "    #eps_decay = 0.0\n",
    "    #eps_min = 0.01\n",
    "    alpha = 0.2\n",
    "\n",
    "    NUM_ITERATIONS = 10\n",
    "    WINNING_MEAN = 170\n",
    "    EPS_ANALYSIS = 0\n",
    "    ALPHA_ANALYSIS = 1\n",
    "    DISCOUNT_ANALYSIS = 2\n",
    "    mode = EPS_ANALYSIS\n",
    "\n",
    "    # number of discrete states\n",
    "    n_bins = 8\n",
    "    n_bins_angle = 10\n",
    "\n",
    "    # discrete states for each variable\n",
    "    cart_position_bins = pandas.cut([-2.4, 2.4], bins=n_bins, retbins=True)[1][1:-1]\n",
    "    cart_velocity_bins = pandas.cut([-1, 1], bins=n_bins, retbins=True)[1][1:-1]\n",
    "    pole_angle_bins = pandas.cut([-2, 2], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "    angle_rate_bins = pandas.cut([-3.5, 3.5], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "\n",
    "    array_eps_to_conv = []\n",
    "    if (mode == EPS_ANALYSIS):\n",
    "        eval_array = [0.001,0.003,0.006,0.01,0.013]\n",
    "    elif (mode == ALPHA_ANALYSIS):\n",
    "        eval_array = [0.1,0.2,0.3,0.5,0.7]\n",
    "    else:\n",
    "        eval_array = [0.999,0.8,0.7,0.6,0.5]\n",
    "\n",
    "    for t in range(NUM_ITERATIONS):\n",
    "        array_temp = []\n",
    "        print(t)\n",
    "\n",
    "        for hyperparameter in eval_array:\n",
    "            if (mode == EPS_ANALYSIS):\n",
    "                has_converged, episodes_to_converge = q_learning_control(env, max_num_episodes, discount, hyperparameter, alpha)\n",
    "            elif (mode == ALPHA_ANALYSIS):\n",
    "                has_converged, episodes_to_converge = q_learning_control(env, max_num_episodes, discount, eps, hyperparameter)\n",
    "            else:\n",
    "                has_converged, episodes_to_converge = q_learning_control(env, max_num_episodes, hyperparameter, eps, alpha)\n",
    "\n",
    "            array_temp.append(episodes_to_converge)\n",
    "            print(hyperparameter, \")the system \", \"has converged\" if (has_converged) else \"hasn't converged\", \" after \", episodes_to_converge, \" episodes.\")\n",
    "\n",
    "        array_eps_to_conv.append(array_temp)\n",
    "\n",
    "    array_eps_to_conv = np.array(array_eps_to_conv)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    if (mode == EPS_ANALYSIS):\n",
    "        title = r'$\\gamma = {:.3f}; \\alpha = {:.3f}$'.format(discount, alpha)\n",
    "        label = r'$\\epsilon$'\n",
    "        name = 'eps_sensibility.png'\n",
    "        ax.set_ylim((0,2000))\n",
    "    elif (mode == ALPHA_ANALYSIS):\n",
    "        title = r'$\\gamma = {:.3f}; \\epsilon = {:.3f}$'.format(discount, eps)\n",
    "        label = r'$\\alpha$'\n",
    "        name = 'lr_sensibility.png'\n",
    "        ax.set_ylim((0,1000))\n",
    "    else:\n",
    "        title = r'$\\alpha = {:.3f}; \\epsilon = {:.3f}$'.format(alpha, eps)\n",
    "        label = r'$1-\\gamma$'\n",
    "        eval_array = [np.around(1-disc, decimals=4) for disc in eval_array]\n",
    "        name = 'discount_sensibility.png'\n",
    "        ax.set_ylim((0,1500))\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('episodes to converge')\n",
    "    ax.set_xlabel(label)\n",
    "    ax.boxplot(array_eps_to_conv, labels=eval_array)\n",
    "    plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.1 )the system  has converged  after  152  episodes.\n",
      "0.2 )the system  has converged  after  596  episodes.\n",
      "0.3 )the system  has converged  after  153  episodes.\n",
      "0.5 )the system  has converged  after  310  episodes.\n",
      "0.7 )the system  has converged  after  505  episodes.\n",
      "1\n",
      "0.1 )the system  has converged  after  241  episodes.\n",
      "0.2 )the system  has converged  after  131  episodes.\n",
      "0.3 )the system  has converged  after  391  episodes.\n",
      "0.5 )the system  has converged  after  286  episodes.\n",
      "0.7 )the system  has converged  after  372  episodes.\n",
      "2\n",
      "0.1 )the system  has converged  after  1043  episodes.\n",
      "0.2 )the system  has converged  after  1079  episodes.\n",
      "0.3 )the system  has converged  after  690  episodes.\n",
      "0.5 )the system  has converged  after  709  episodes.\n",
      "0.7 )the system  has converged  after  847  episodes.\n",
      "3\n",
      "0.1 )the system  has converged  after  826  episodes.\n",
      "0.2 )the system  has converged  after  633  episodes.\n",
      "0.3 )the system  has converged  after  696  episodes.\n",
      "0.5 )the system  has converged  after  134  episodes.\n",
      "0.7 )the system  has converged  after  395  episodes.\n",
      "4\n",
      "0.1 )the system  has converged  after  698  episodes.\n",
      "0.2 )the system  has converged  after  640  episodes.\n",
      "0.3 )the system  has converged  after  508  episodes.\n",
      "0.5 )the system  has converged  after  923  episodes.\n",
      "0.7 )the system  has converged  after  170  episodes.\n",
      "5\n",
      "0.1 )the system  has converged  after  811  episodes.\n",
      "0.2 )the system  has converged  after  368  episodes.\n",
      "0.3 )the system  has converged  after  667  episodes.\n",
      "0.5 )the system  has converged  after  135  episodes.\n",
      "0.7 )the system  has converged  after  276  episodes.\n",
      "6\n",
      "0.1 )the system  has converged  after  514  episodes.\n",
      "0.2 )the system  has converged  after  380  episodes.\n",
      "0.3 )the system  has converged  after  884  episodes.\n",
      "0.5 )the system  has converged  after  908  episodes.\n",
      "0.7 )the system  has converged  after  300  episodes.\n",
      "7\n",
      "0.1 )the system  has converged  after  416  episodes.\n",
      "0.2 )the system  has converged  after  524  episodes.\n",
      "0.3 )the system  has converged  after  651  episodes.\n",
      "0.5 )the system  has converged  after  649  episodes.\n",
      "0.7 )the system  has converged  after  380  episodes.\n",
      "8\n",
      "0.1 )the system  has converged  after  748  episodes.\n",
      "0.2 )the system  has converged  after  214  episodes.\n",
      "0.3 )the system  has converged  after  817  episodes.\n",
      "0.5 )the system  has converged  after  880  episodes.\n",
      "0.7 )the system  has converged  after  479  episodes.\n",
      "9\n",
      "0.1 )the system  has converged  after  1261  episodes.\n",
      "0.2 )the system  has converged  after  1010  episodes.\n",
      "0.3 )the system  has converged  after  499  episodes.\n",
      "0.5 )the system  has converged  after  222  episodes.\n",
      "0.7 )the system  has converged  after  625  episodes.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Q-learning algorithm implementation.\"\"\"\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import gym\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_eps_greedy_policy(Q, eps, n):\n",
    "    \"\"\"Define a policy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : array_like (n_states x n_actions)\n",
    "        Action-values.\n",
    "    eps : float\n",
    "        Eps-greedy factor.\n",
    "    n : integer\n",
    "        Number of actions.\n",
    "    Returns\n",
    "    -------\n",
    "    policy : function\n",
    "        Function that returns actions given an input state (the present state).\n",
    "    \"\"\"\n",
    "    def policy(state):\n",
    "        \"\"\"Define a set of actions.\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : tuple\n",
    "            Present state on which depends our next action.\n",
    "        Retuns\n",
    "        ------\n",
    "        A : array\n",
    "            Probabilities for actions in the set of possible actions to be taken.\n",
    "        \"\"\"\n",
    "        A = np.ones(n, dtype=float) * eps/n\n",
    "        best = np.argmax(Q[state])\n",
    "        A[best] += 1 - eps\n",
    "        return A\n",
    "    return policy\n",
    "\n",
    "def q_learning_control(env, max_num_episodes, discount=1.0, eps=0.99, alpha=0.05):\n",
    "    \"\"\"Q-learning control.\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : OpenAI gym environment\n",
    "        Environment which will be used in the simulation (CartPole-v0).\n",
    "    max_num_episodes : int\n",
    "        Max number of episodes to converge.\n",
    "    discount : float\n",
    "        Discount factor.\n",
    "    eps : float\n",
    "        Exploration rate (since we are using an epsilon-greedy policy).\n",
    "    eps_decay : float\n",
    "        Exploration rate decay over episodes.\n",
    "    eps_min : float\n",
    "        Min exploration rate reachable.\n",
    "    Returns\n",
    "    -------\n",
    "    converged : bool\n",
    "        True if the algorithm converged, False otherwise.\n",
    "    num_episodes : int\n",
    "        Number of episodes to converge.\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    possible_actions = np.arange(env.action_space.n)\n",
    "    converged = False\n",
    "    returns = []\n",
    "\n",
    "    for num_episodes in range(max_num_episodes):\n",
    "        totalreward = 0     # total reward in this episodes (+1 each step)\n",
    "        state = build_state(env.reset())\n",
    "        policy = make_eps_greedy_policy(Q, eps, env.action_space.n)\n",
    "\n",
    "        for t in itertools.count():\n",
    "            probs = policy(state)\n",
    "            action = rnd.choice(possible_actions, p=probs)\n",
    "\n",
    "            # take the current action and observe the reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = build_state(next_state)\n",
    "\n",
    "            # if the cartpole system fell down during this episode\n",
    "            if (done):\n",
    "                # high penalization helps the convergence\n",
    "                Q[state][action] += -200\n",
    "                totalreward += reward\n",
    "                returns.append(totalreward)\n",
    "                break\n",
    "\n",
    "            # update Q-values using Q-learning update rule\n",
    "            max_action = np.argmax(Q[next_state])\n",
    "            Q[state][action] = Q[state][action] + alpha*(reward + discount*Q[next_state][max_action] - Q[state][action])\n",
    "\n",
    "            # append this step's reward\n",
    "            totalreward += reward\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # we are not decaying in the sensibility analysis\n",
    "        #if i%100 == 0:\n",
    "        #    eps *= eps_decay\n",
    "        #    if (eps < eps_min):\n",
    "        #        eps = eps_min\n",
    "\n",
    "        # winning condition: last 100 episodes have a mean of WINNING_MEAN total reward\n",
    "        mean = np.mean(returns[-100:])\n",
    "        if mean >= WINNING_MEAN:\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "    return converged, num_episodes\n",
    "\n",
    "def build_state(state):\n",
    "    \"\"\"Discretize the state returned by the environment.\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : tuple\n",
    "        State returned by OpenAI gym environment.\n",
    "    Returns\n",
    "    -------\n",
    "    _ : tuple\n",
    "        The correspondent discrete state.\n",
    "    \"\"\"\n",
    "    return (np.digitize([state[0]], cart_position_bins)[0],\n",
    "            np.digitize([state[1]], cart_velocity_bins)[0],\n",
    "            np.digitize([state[2]], pole_angle_bins)[0],\n",
    "            np.digitize([state[3]], angle_rate_bins)[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v0')\n",
    "    max_num_episodes = 6000\n",
    "    discount = 0.999\n",
    "    eps = 0.001\n",
    "    #eps_decay = 0.0\n",
    "    #eps_min = 0.01\n",
    "    alpha = 0.2\n",
    "\n",
    "    NUM_ITERATIONS = 10\n",
    "    WINNING_MEAN = 170\n",
    "    EPS_ANALYSIS = 0\n",
    "    ALPHA_ANALYSIS = 1\n",
    "    DISCOUNT_ANALYSIS = 2\n",
    "    mode = ALPHA_ANALYSIS\n",
    "\n",
    "    # number of discrete states\n",
    "    n_bins = 8\n",
    "    n_bins_angle = 10\n",
    "\n",
    "    # discrete states for each variable\n",
    "    cart_position_bins = pandas.cut([-2.4, 2.4], bins=n_bins, retbins=True)[1][1:-1]\n",
    "    cart_velocity_bins = pandas.cut([-1, 1], bins=n_bins, retbins=True)[1][1:-1]\n",
    "    pole_angle_bins = pandas.cut([-2, 2], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "    angle_rate_bins = pandas.cut([-3.5, 3.5], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "\n",
    "    array_eps_to_conv = []\n",
    "    if (mode == EPS_ANALYSIS):\n",
    "        eval_array = [0.001,0.003,0.006,0.01,0.013]\n",
    "    elif (mode == ALPHA_ANALYSIS):\n",
    "        eval_array = [0.1,0.2,0.3,0.5,0.7]\n",
    "    else:\n",
    "        eval_array = [0.999,0.8,0.7,0.6,0.5]\n",
    "\n",
    "    for t in range(NUM_ITERATIONS):\n",
    "        array_temp = []\n",
    "        print(t)\n",
    "\n",
    "        for hyperparameter in eval_array:\n",
    "            if (mode == EPS_ANALYSIS):\n",
    "                has_converged, episodes_to_converge = q_learning_control(env, max_num_episodes, discount, hyperparameter, alpha)\n",
    "            elif (mode == ALPHA_ANALYSIS):\n",
    "                has_converged, episodes_to_converge = q_learning_control(env, max_num_episodes, discount, eps, hyperparameter)\n",
    "            else:\n",
    "                has_converged, episodes_to_converge = q_learning_control(env, max_num_episodes, hyperparameter, eps, alpha)\n",
    "\n",
    "            array_temp.append(episodes_to_converge)\n",
    "            print(hyperparameter, \")the system \", \"has converged\" if (has_converged) else \"hasn't converged\", \" after \", episodes_to_converge, \" episodes.\")\n",
    "\n",
    "        array_eps_to_conv.append(array_temp)\n",
    "\n",
    "    array_eps_to_conv = np.array(array_eps_to_conv)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    if (mode == EPS_ANALYSIS):\n",
    "        title = r'$\\gamma = {:.3f}; \\alpha = {:.3f}$'.format(discount, alpha)\n",
    "        label = r'$\\epsilon$'\n",
    "        name = 'eps_sensibility.png'\n",
    "        ax.set_ylim((0,2000))\n",
    "    elif (mode == ALPHA_ANALYSIS):\n",
    "        title = r'$\\gamma = {:.3f}; \\epsilon = {:.3f}$'.format(discount, eps)\n",
    "        label = r'$\\alpha$'\n",
    "        name = 'lr_sensibility.png'\n",
    "        ax.set_ylim((0,1000))\n",
    "    else:\n",
    "        title = r'$\\alpha = {:.3f}; \\epsilon = {:.3f}$'.format(alpha, eps)\n",
    "        label = r'$1-\\gamma$'\n",
    "        eval_array = [np.around(1-disc, decimals=4) for disc in eval_array]\n",
    "        name = 'discount_sensibility.png'\n",
    "        ax.set_ylim((0,1500))\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('episodes to converge')\n",
    "    ax.set_xlabel(label)\n",
    "    ax.boxplot(array_eps_to_conv, labels=eval_array)\n",
    "    plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
